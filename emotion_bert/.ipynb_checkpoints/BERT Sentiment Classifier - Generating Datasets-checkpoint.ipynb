{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "### https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GeForce GTX 1060\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = torch.cuda.current_device()\n",
    "    torch.cuda.device(dev)\n",
    "    print(\"Using\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the data [DailyDialog] to make it task specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset daily_dialog (/home/sambit/.cache/huggingface/datasets/daily_dialog/default/1.0.0/c03444008e9508b8b76f1f6793742d37d5e5f83364f8d573c2747bff435ea55c)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['dialog', 'act', 'emotion'],\n",
      "        num_rows: 11118\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['dialog', 'act', 'emotion'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['dialog', 'act', 'emotion'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('daily_dialog')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('act', [[3, 4, 2, 2, 2, 3, 4, 1, 3, 4], [2, 1, 2, 2, 1, 1]]),\n",
       "             ('dialog',\n",
       "              [['Say , Jim , how about going for a few beers after dinner ? ',\n",
       "                ' You know that is tempting but is really not good for our fitness . ',\n",
       "                ' What do you mean ? It will help us to relax . ',\n",
       "                \" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? \",\n",
       "                \" I guess you are right.But what shall we do ? I don't feel like sitting at home . \",\n",
       "                ' I suggest a walk over to the gym where we can play singsong and meet some of our friends . ',\n",
       "                \" That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . \",\n",
       "                ' Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . ',\n",
       "                \" Good.Let ' s go now . \",\n",
       "                ' All right . '],\n",
       "               ['Can you do push-ups ? ',\n",
       "                \" Of course I can . It's a piece of cake ! Believe it or not , I can do 30 push-ups a minute . \",\n",
       "                \" Really ? I think that's impossible ! \",\n",
       "                ' You mean 30 push-ups ? ',\n",
       "                ' Yeah ! ',\n",
       "                \" It's easy . If you do exercise everyday , you can make it , too . \"]]),\n",
       "             ('emotion',\n",
       "              [[0, 0, 0, 0, 0, 0, 4, 4, 4, 4], [0, 0, 6, 0, 0, 0]])])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_into_lists(mode):\n",
    "    global dataset\n",
    "    temp_data = dataset[mode]\n",
    "    rows = []\n",
    "    max_length_previous = 0\n",
    "    max_pidx = -1\n",
    "    max_length_current = 0\n",
    "    max_cidx = -1\n",
    "    \n",
    "    for i, d in enumerate(temp_data):\n",
    "        acts = d[\"act\"]\n",
    "        dialogs = d[\"dialog\"]\n",
    "        emotions = d[\"emotion\"]\n",
    "        \n",
    "        for idx in range(1, len(acts)):\n",
    "            rows.append([dialogs[idx-1], dialogs[idx], acts[idx], emotions[idx]])\n",
    "            \n",
    "    df = pd.DataFrame(data=rows, columns=['previous_dialog', 'current_dialog', 'act', 'emotion'])\n",
    "    df.to_csv(\"data/daily_dialog/csv/\"+mode+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNCOMMENT IF THE DATA SPLITTING PROCESS IS CHANGED\n",
    "split_data_into_lists(\"test\")\n",
    "split_data_into_lists(\"validation\")\n",
    "split_data_into_lists(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ohe_act = OneHotEncoder(handle_unknown='ignore')\n",
    "X = [[1], [2], [3], [4]]\n",
    "ohe_act.fit(X)\n",
    "\n",
    "# { 1: informï¼Œ2: question, 3: directive, 4: commissive }\n",
    "\n",
    "ohe_emo = OneHotEncoder(handle_unknown='ignore')\n",
    "X = [[0], [1], [2], [3], [4], [5], [6]]\n",
    "ohe_emo.fit(X)\n",
    "\n",
    "# { 0: no emotion, 1: anger, 2: disgust, 3: fear, 4: happiness, 5: sadness, 6: surprise}\n",
    "\n",
    "print(ohe_act.transform([[1]]).toarray())\n",
    "print(ohe_emo.transform([[4]]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(mode):\n",
    "    df = pd.read_csv(\"data/daily_dialog/csv/\"+mode+\".csv\")\n",
    "    return df\n",
    "\n",
    "# Get the dialog from the generated dataframe\n",
    "def get_cell(df, row, column_name):\n",
    "    return df.loc[df.index[row], column_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_csv(\"train\")\n",
    "val = load_csv(\"validation\")\n",
    "test = load_csv(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  You know that is tempting but is really not good for our fitness . \n",
      "Tokenised: ['you', 'know', 'that', 'is', 'tempting', 'but', 'is', 'really', 'not', 'good', 'for', 'our', 'fitness', '.']\n",
      "Token ID: [2017, 2113, 2008, 2003, 23421, 2021, 2003, 2428, 2025, 2204, 2005, 2256, 10516, 1012]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "sentence = get_cell(train, 0, \"current_dialog\")\n",
    "print(\"Original:\", sentence)\n",
    "tokenized = tokenizer.tokenize(sentence)\n",
    "print(\"Tokenised:\", tokenized)\n",
    "tokenided = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "print(\"Token ID:\", tokenided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the current and previous dialog for a specific row\n",
    "# Can return emotion data or act data depending on \"mode\"\n",
    "\n",
    "max_previous_length = 0\n",
    "max_current_length = 0\n",
    "\n",
    "def get_processed_row(df, row, mode=\"emo\"):\n",
    "    global  max_previous_length\n",
    "    global max_current_length\n",
    "    \n",
    "    sentence = get_cell(df, row, \"previous_dialog\")\n",
    "    tokenized = tokenizer.tokenize(sentence)\n",
    "    tokenided_p = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    \n",
    "    sentence = get_cell(df, row, \"current_dialog\")\n",
    "    tokenized = tokenizer.tokenize(sentence)\n",
    "    tokenided_c = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    \n",
    "    if len(tokenided_c) > max_current_length:\n",
    "        max_current_length = len(tokenided_c)\n",
    "    \n",
    "    if len(tokenided_p) > max_previous_length:\n",
    "        max_previous_length = len(tokenided_p)\n",
    "    \n",
    "    label = None\n",
    "    \n",
    "    if mode == \"act\":\n",
    "        label = ohe_act.transform([[get_cell(df, row, \"act\")]]).toarray().flatten().tolist()\n",
    "    else:\n",
    "        label = ohe_emo.transform([[get_cell(df, row, \"emotion\")]]).toarray().flatten().tolist()\n",
    "    \n",
    "    return tokenided_p, tokenided_c, label\n",
    "\n",
    "\n",
    "# Save the sentence with one hot encoded labels (can be used with encode_plus)\n",
    "\n",
    "def get_unprocessed_row(df, row, mode=\"emo\"):\n",
    "    global  max_previous_length\n",
    "    global max_current_length\n",
    "    \n",
    "    sentence_p = get_cell(df, row, \"previous_dialog\")\n",
    "    sentence_c = get_cell(df, row, \"current_dialog\")\n",
    "    \n",
    "    label = None\n",
    "    \n",
    "    if mode == \"act\":\n",
    "        label = get_cell(df, row, \"act\")\n",
    "    else:\n",
    "        label = get_cell(df, row, \"emotion\")\n",
    "    \n",
    "    return sentence_p, sentence_c, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save tokenised data\n",
    "\n",
    "def save_to_pkl(data, mode, dataset=\"emo\"):\n",
    "    with open(\"data/daily_dialog/pkl/\"+mode+\"_\"+dataset+\".pkl\", \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def save_to_sent(data, mode, dataset=\"emo\"):\n",
    "    with open(\"data/daily_dialog/sent/\"+mode+\"_\"+dataset+\".pkl\", \"wb\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating PKLs for Emotion Data & Act Data (Sentences are tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Input Lengths:\n",
      "Previous Sentence: 275\n",
      "Current Sentence: 294\n"
     ]
    }
   ],
   "source": [
    "max_previous_length = 0\n",
    "max_current_length = 0\n",
    "\n",
    "train_data = []\n",
    "for i in range(train.shape[0]):\n",
    "    train_data.append(get_processed_row(train, i))\n",
    "save_to_pkl(train_data, \"train\")\n",
    "\n",
    "val_data = []\n",
    "for i in range(val.shape[0]):\n",
    "    val_data.append(get_processed_row(val, i))\n",
    "save_to_pkl(val_data, \"val\")\n",
    "\n",
    "test_data = []\n",
    "for i in range(test.shape[0]):\n",
    "    test_data.append(get_processed_row(test, i))\n",
    "save_to_pkl(test_data, \"test\")\n",
    "\n",
    "\n",
    "max_previous_length = 0\n",
    "max_current_length = 0\n",
    "\n",
    "train_data = []\n",
    "for i in range(train.shape[0]):\n",
    "    train_data.append(get_processed_row(train, i, \"act\"))\n",
    "save_to_pkl(train_data, \"train\", \"act\")\n",
    "\n",
    "val_data = []\n",
    "for i in range(val.shape[0]):\n",
    "    val_data.append(get_processed_row(val, i, \"act\"))\n",
    "save_to_pkl(val_data, \"val\", \"act\")\n",
    "\n",
    "test_data = []\n",
    "for i in range(test.shape[0]):\n",
    "    test_data.append(get_processed_row(test, i, \"act\"))\n",
    "save_to_pkl(test_data, \"test\", \"act\")\n",
    "    \n",
    "print(\"Max Input Lengths:\")\n",
    "print(\"Previous Sentence:\", max_previous_length)\n",
    "print(\"Current Sentence:\", max_current_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating PKLs for Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emo data\n",
    "\n",
    "train_data = []\n",
    "for i in range(train.shape[0]):\n",
    "    train_data.append(get_unprocessed_row(train, i))\n",
    "save_to_sent(train_data, \"train\")\n",
    "\n",
    "val_data = []\n",
    "for i in range(val.shape[0]):\n",
    "    val_data.append(get_unprocessed_row(val, i))\n",
    "save_to_sent(val_data, \"val\")\n",
    "\n",
    "test_data = []\n",
    "for i in range(test.shape[0]):\n",
    "    test_data.append(get_unprocessed_row(test, i))\n",
    "save_to_sent(test_data, \"test\")\n",
    "\n",
    "\n",
    "# Act data\n",
    "\n",
    "train_data = []\n",
    "for i in range(train.shape[0]):\n",
    "    train_data.append(get_unprocessed_row(train, i, \"act\"))\n",
    "save_to_sent(train_data, \"train\", \"act\")\n",
    "\n",
    "val_data = []\n",
    "for i in range(val.shape[0]):\n",
    "    val_data.append(get_unprocessed_row(val, i, \"act\"))\n",
    "save_to_sent(val_data, \"val\", \"act\")\n",
    "\n",
    "test_data = []\n",
    "for i in range(test.shape[0]):\n",
    "    test_data.append(get_unprocessed_row(test, i, \"act\"))\n",
    "save_to_sent(test_data, \"test\", \"act\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
